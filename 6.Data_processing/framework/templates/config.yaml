# DQ Transformation Framework Configuration Template
#
# This file defines the configuration for data transformations with 
# enforced data quality checks using Great Expectations.

# Transformation Configuration
transformation:
  # Unique identifier for this transformation
  id: "my_transformation"
  
  # Description of the transformation
  description: "Sample data transformation with DQ checks"
  
  # Transformation engine type: dbt, sql, spark, python
  engine_type: "dbt"
  
  # Source tables/datasets
  source_tables:
    - "raw_data.customers"
    - "raw_data.orders"
  
  # Target tables/datasets
  target_tables:
    - "processed_data.customer_metrics"
  
  # Engine-specific configuration path
  config_path: "./dbt_project"
  
  # Additional metadata
  metadata:
    owner: "data_team"
    version: "1.0"
    tags: ["customer", "metrics"]

# Data Quality Configuration
data_quality:
  # Pre-transformation checks
  pre_checks:
    enabled: true
    check_names:
      - "completeness_check"
      - "schema_validation"
      - "data_freshness"
    
    # Fail transformation if pre-checks fail
    fail_on_error: true
  
  # Post-transformation checks
  post_checks:
    enabled: true
    check_names:
      - "accuracy_check"
      - "consistency_check"
      - "completeness_check"
    
    # Fail transformation if post-checks fail
    fail_on_error: true
  
  # Great Expectations configuration
  gx_config:
    # Path to data contract (used to generate GX suites if needed)
    contract_path: "../../1.Data_contract/contract.yaml"
    
    # Path to pre-generated GX suites from Step 5 (prioritized)
    suites_path: "../../5.GX_code/suites_cli"
    
    # Custom GX configuration directory (optional)
    gx_dir: "./gx"
    
    # Reports output directory
    reports_dir: "./reports"

# Engine-specific configurations
engines:
  dbt:
    # dbt project directory
    project_dir: "./dbt_project"
    
    # dbt profiles directory
    profiles_dir: "~/.dbt"
    
    # dbt target (dev, prod, etc.)
    target: "dev"
    
    # Additional dbt options
    options:
      vars:
        start_date: "2024-01-01"
        end_date: "2024-12-31"
  
  sql:
    # Database connection string
    connection_string: "duckdb:///data/warehouse.db"
    
    # SQL scripts directory
    scripts_dir: "./sql"
    
    # Execution order file (optional)
    execution_order: "./sql/execution_order.txt"
  
  spark:
    # Spark application configuration
    app_name: "dq_transformation"
    
    # Spark master URL
    master: "local[*]"
    
    # Spark configuration
    spark_config:
      "spark.sql.adaptive.enabled": "true"
      "spark.sql.adaptive.coalescePartitions.enabled": "true"
    
    # Python script or JAR file to execute
    script_path: "./spark/transform.py"
  
  python:
    # Python script to execute
    script_path: "./python/transform.py"
    
    # Python requirements file
    requirements_file: "./python/requirements.txt"
    
    # Python environment (optional)
    python_env: "venv"

# Environment variables
environment:
  # Database connections
  DB_HOST: "localhost"
  DB_PORT: "5432"
  DB_NAME: "warehouse"
  DB_USER: "data_user"
  # DB_PASSWORD should be set separately for security
  
  # Other environment variables
  DATA_PATH: "/data"
  LOG_LEVEL: "INFO"

# Output configuration
output:
  # Base output directory
  base_dir: "./output"
  
  # Keep intermediate files
  keep_intermediate: false
  
  # Generate lineage information
  generate_lineage: true
  
  # Lineage output format (yaml, json)
  lineage_format: "yaml"

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/transformation.log"
